\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{arxiv}
\usepackage{multirow}
\usepackage{booktabs}   % Already present, good for clean tables
\usepackage{array}      % For better table column control
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage[numbers]{natbib}  % Explicitly use numbers style for citations
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmicx}     % Base package for algorithmic environments
\usepackage{algpseudocode}    % Specific style for pseudocode

% Title
\title{Prime-Adaptive Search (PAS): A Novel Method for Efficient Optimization in Discontinuous Landscapes}

% Author information
\author{
  Sethu Iyer\\
  \texttt{sethuiyer@gmail.com}\\
  \href{https://linkedin.com/in/sethuiyer}{linkedin.com/in/sethuiyer}
}

\begin{document}
\maketitle

\begin{abstract}
Modern optimization problems increasingly involve discontinuous, non-smooth, and multi-modal functions, rendering traditional gradient-based methods ineffective. This paper introduces Prime-Adaptive Search (PAS), a novel iterative optimization technique that leverages prime number properties to adaptively refine the search space. PAS employs a prime-driven partitioning scheme to avoid aliasing and to naturally provide a hierarchical resolution of the search domain. By focusing on function evaluations at prime-partitioned grids and adaptively shrinking the domain around promising candidates, PAS excels in robustly handling non-smooth functions and navigating multi-modal landscapes. We present empirical results from benchmark problems, including discontinuous functions, LeetCode-style "peak-finding," and challenging 2D/3D scenarios. Our findings demonstrate PAS's advantages—adaptive resolution, avoidance of periodic sampling bias, and gradient independence—all culminating in strong performance for a broad range of practical applications, from AI hyperparameter tuning to cryptographic parameter searches.
\end{abstract}

\section{Introduction and Motivation}

\subsection{Challenges in Discontinuous Optimization}

Many real-world optimization tasks involve discontinuities, non-smooth structures, or black-box simulations. In these settings, gradient-based solvers like Newton's Method or quasi-Newton methods fail due to undefined or misleading derivatives. Even popular derivative-free optimizers can stall in complex multi-modal landscapes or require uniform sampling that wastes evaluations.

\subsection{The Potential of Adaptive Searching}

Uniform grid searches are simple and can systematically scan large domains but become exponentially costly in higher dimensions. Adaptive methods—where resolution dynamically changes based on local feedback—provide a more efficient approach. They focus computational efforts in promising areas, allowing them to handle non-smooth functions better than naive uniform grids.

\subsection{Primes for Partitioning}

Prime numbers have interesting mathematical properties, notably lack of simple integer factors, which helps avoid aliasing. Partitioning intervals with primes means the sampling pattern changes unpredictably each iteration, preventing alignment with periodic function features. This ensures robust exploration and a natural coarse-to-fine approach as we shift from smaller primes to larger primes.

\subsection{PAS: Prime-Adaptive Search}

The Prime-Adaptive Search algorithm incorporates:
\begin{enumerate}
    \item \textbf{Prime Number-Based Grid Sampling}: Using prime $p$ to partition an interval into $p$ sub-intervals.
    \item \textbf{Adaptive Interval Refinement}: Resizing the search domain around the best-found solution each iteration.
    \item \textbf{Dynamic Prime-Index Adjustment}: Modifying the prime index each iteration based on local progress, effectively controlling resolution.
\end{enumerate}

This paper explores PAS's design, strengths, and empirical performance on an array of discontinuous and multi-modal test problems.

\section{Algorithm Development \& Core Concepts}

\subsection{Prime-Adaptive Search Steps}

Let us consider PAS in one dimension for clarity; the same logic extends to multi-dimensional spaces:

\begin{enumerate}
    \item \textbf{Initialization}
    \begin{itemize}
        \item Define an initial interval $[x_{\min}, x_{\max}]$.
        \item Choose an initial prime index $p_{idx}$ and a maximum iteration count $k_{\max}$.
        \item (Optional) Set a tolerance $\varepsilon$ for convergence criteria.
    \end{itemize}
    
    \item \textbf{Prime-Based Partitioning}
    \begin{itemize}
        \item Obtain the prime $p$ associated with $p_{idx}$.
        \item Generate $p$ points linearly between $[x_{\min}, x_{\max}]$.
        \item Evaluate the objective function $f(x)$ at these grid points.
    \end{itemize}
    
    \item \textbf{Best Candidate Selection}
    \begin{itemize}
        \item Identify $x_{\text{best}}$ with the optimal $f(x)$ (e.g., minimal).
    \end{itemize}
    
    \item \textbf{Refine Range}
    \begin{itemize}
        \item Shrink the search interval around $x_{\text{best}}$. For instance,  
        $\Delta = \frac{(x_{\max} - x_{\min})}{p - 1} \quad \text{or} \quad \Delta = \alpha (x_{\max} - x_{\min})$
        \item Update $\,[x_{\min}, x_{\max}] \leftarrow [\,x_{\text{best}} - \Delta/2,\, x_{\text{best}} + \Delta/2\,]\,.$
    \end{itemize}
    
    \item \textbf{Adjust Prime Index}
    \begin{itemize}
        \item Based on $|f(x_{\text{best}})|$ or local search feedback, increase $p_{idx}$ if a finer resolution is needed, or decrease if the algorithm is overshooting.
    \end{itemize}
    
    \item \textbf{Termination}
    \begin{itemize}
        \item Repeat steps 2–5 until the maximum iteration $k_{\max}$ is reached or $|f(x_{\text{best}})| < \varepsilon$.
    \end{itemize}
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig1_pas_steps_1d.png}
    \caption{Visualization of PAS algorithm iterations on a 1D function. Each panel shows a different iteration with increasing prime values (p=5, p=7, p=11), demonstrating how the search domain shrinks around promising regions while grid resolution increases.}
    \label{fig:pas_steps_1d}
\end{figure}

\subsection{Multi-Dimensional Extension}

In multi-dimensional problems (e.g., 3D), the algorithm can:
\begin{itemize}
    \item Partition each dimension with a (possibly distinct) prime, yielding a prime-based 3D grid.
    \item Evaluate $f$ at these grid points, locate the best $(x,y,z)$.
    \item Shrink the bounding box around that best candidate.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig4_prime_grid_sampling.png}
    \caption{Prime-based grid sampling across iterations, showing how different prime numbers create different grid patterns (p=5, p=7, p=11, p=13). Red boxes indicate the zoomed regions for subsequent iterations, demonstrating the adaptive refinement process in 2D.}
    \label{fig:prime_grid_sampling}
\end{figure}

\subsection{Pseudocode}

\begin{algorithm}
\caption{Prime-Adaptive Search (PAS)}
\begin{algorithmic}[1]
\Procedure{PAS}{$f$, $[x_{min}, x_{max}]$, $max\_iters$, $p\_index\_init$, $tol$}
    \State $p\_index \gets p\_index\_init$
    \For{$k = 1$ to $max\_iters$}
        \State $p \gets prime(p\_index)$
        \State $X \gets linspace(x_{min}, x_{max}, p)$
        \State $F \gets [f(x) \text{ for } x \text{ in } X]$
        \State $x_{best} \gets X[argmin(F)]$
        \State $\Delta \gets (x_{max} - x_{min}) / (p - 1)$
        \State $x_{min}, x_{max} \gets x_{best} - \Delta/2, x_{best} + \Delta/2$
        \If{$|f(x_{best})| < tol$} 
            \State \textbf{return} $x_{best}$
        \EndIf
        \State $p\_index \gets update\_rule(p\_index, f(x_{best}))$
    \EndFor
    \State \textbf{return} $x_{best}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Key Strengths and Weaknesses of PAS}

\subsection{Strengths}

\begin{enumerate}
    \item \textbf{Robustness to Discontinuities}\\
    PAS can handle piecewise or abrupt step functions with ease, since it does not rely on smooth gradients.
    
    \item \textbf{No Gradient Reliance}\\
    Black-box scenarios (e.g., simulations) are feasible with PAS, as it only needs function evaluations.
    
    \item \textbf{Adaptive Resolution}\\
    The prime index can be increased or decreased to zoom in or broaden the search area.
    
    \item \textbf{Avoids Aliasing}\\
    By partitioning via primes, PAS avoids repeated alignment with periodic function features.
    
    \item \textbf{Near-Binary-Search Efficiency}\\
    On certain problems (peak-finding), PAS performed similarly to binary search, but without requiring monotonic assumptions.
\end{enumerate}

\subsection{Weaknesses}

\begin{enumerate}
    \item \textbf{Less Competitive on Smooth Functions}\\
    On well-behaved, unimodal, differentiable functions, classical methods (Newton, Secant) converge faster.
    
    \item \textbf{High Evaluation Cost}\\
    For highly discontinuous or large-dimensional spaces, naive PAS might do many function evaluations.
    
    \item \textbf{Prime Index Sensitivity}\\
    The update rule can significantly affect performance. Overly aggressive or conservative changes hamper convergence.
    
    \item \textbf{Dimensionality Scaling}\\
    Partitioning each dimension with prime-based grids can lead to exponential blow-up in evaluations (though hybrid/advanced approaches mitigate this).
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig5_comparison_table.png}
    \caption{Comparison of optimization methods across key performance metrics. Green cells indicate advantages, orange/red cells indicate limitations. Note PAS's strong performance on discontinuity handling and absence of gradient requirements.}
    \label{fig:comparison_table}
\end{figure}

\section{Experimental Results}

We evaluate DAPS (Dimensionally Adaptive Prime Search) against standard optimization methods—Bisection, Newton, Secant, Nelder-Mead, Grid Search, and Hybrid PAS—on a suite of benchmark problems designed to test performance in discontinuous, non-smooth, and high-dimensional landscapes.

\begin{table*}[t]
\caption{Benchmark Results Comparing Various Optimization Methods}
\label{tab:benchmark-results}
\centering
\begin{tabular}{@{}lccccl@{}}
\toprule
\textbf{Benchmark Problem} & \textbf{Method} & \textbf{Iterations} & \textbf{Function Evals} & \textbf{Final Error} & \textbf{Remarks} \\
\midrule
\multirow{3}{*}{\textbf{Root Finding:} $e^x = \frac{4\pi}{\sqrt{3}}$} & Bisection & 17 & 18 & $< 10^{-6}$ & Fast convergence \\
& Newton & 5 & 6 & $< 10^{-8}$ & Best for smooth \\
& \textbf{PAS} & 3 & 109 & 0.003 & Slower but robust \\
\midrule
\multirow{2}{*}{\textbf{Step Discontinuity}} & Nelder-Mead & 24 & 80 & Missed jump & Trapped near discontinuity \\
& \textbf{Hybrid PAS} & 5 & 65 & Jump captured & ~80\% fewer evals \\
\midrule
\multirow{2}{*}{\textbf{Sawtooth Discontinuous}} & Nelder-Mead & 31 & 120 & High error & Failed at periodic jumps \\
& \textbf{Hybrid PAS} & 7 & 95 & Peak located & Efficient despite pattern \\
\midrule
\multirow{2}{*}{\textbf{"Peak Element" (LeetCode)}} & Binary Search & 5 & 5 & Found peak & Best for monotonic \\
& \textbf{PGDS} & 6 & 49 & Found peak & No monotonicity assumption \\
\midrule
\textbf{"Mountain Peak" (3L-2R Rule)} & \textbf{PGDS} & 1 & 31 & Correct peak & Shines in complex peaks \\
\midrule
\multirow{2}{*}{\textbf{Recursive Fractal Cliff Valley (3D)}} & Nelder-Mead & 60 & 350 & Local minima & Failed \\
& \textbf{PGDS} & 12 & 220 & Global valley & 3D benchmark passed \\
\midrule
\multirow{2}{*}{\textbf{3D Oscillatory Basin}} & DIRECT & 80 & 400 & Semi-optimal & Oscillations challenge \\
& \textbf{PGDS} & 15 & 250 & Correct basin & Adaptive grid helped \\
\midrule
\multirow{2}{*}{\textbf{3D Rosenbrock Extended}} & Dual Annealing & 90 & 500 & Good but noisy & Standard heuristic \\
& \textbf{PGDS} & 10 & 200 & High precision & Valley efficiently resolved \\
\midrule
\multirow{2}{*}{\textbf{Integer Solution:} $ae^x - bx^2 = 18.23$} & Brute-force & N/A & 5000 & Found after 4900 & Expensive \\
& \textbf{PGDS} & 4 & 75 & Correct integers & Massive speed-up \\
\midrule
\multirow{2}{*}{\textbf{Discontinuous Solar Farm Constraint}} & Nelder-Mead & 50 & 200 & Dead zone & Poor handling \\
& \textbf{PGDS} & 8 & 110 & Optimal config & Efficient at constraints \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig2_discontinuous_comparison.png}
    \caption{Comparative performance on discontinuous functions. Top row shows the test functions (sawtooth and step functions). Bottom row demonstrates how PAS effectively handles these challenging landscapes compared to gradient-based methods and Nelder-Mead.}
    \label{fig:discontinuous_comparison}
\end{figure}

\subsection{1D Benchmarks}

\subsubsection{Root Finding (Smooth Function)}
\begin{itemize}
    \item \textbf{Objective:} Solve $e^x = 4\pi / \sqrt{3}$.
    \item \textbf{Comparison Methods:} Bisection, Newton, Secant, PAS.
    \item \textbf{Result:} Newton and Secant dominate in speed. PAS is slower and requires more evaluations—demonstrating its relative disadvantage on smooth unimodal tasks.
\end{itemize}

\subsubsection{Discontinuous Functions}
\begin{itemize}
    \item \textbf{Floor \& Sawtooth test:} Large jumps or periodic discontinuities.
    \item \textbf{Finding:} PAS performs robustly, avoiding the pitfalls that hamper gradient-based methods. Hybrid PAS (adding local discontinuity detection) drastically reduces evaluations compared to naive uniform sampling.
\end{itemize}

\subsubsection{Peak-Finding}
\begin{itemize}
    \item \textbf{LeetCode style "peak element" or "mountain peak" definitions.}
    \item \textbf{Finding:} PAS identified valid peaks in ~16–17 iterations, on par with binary search in iteration count—without monotonic assumptions. Showcases PAS's adaptive grid's flexibility.
\end{itemize}

\subsection{High Iteration Experiments}

On certain problems, we tested PAS up to 50+ iterations.
\begin{itemize}
    \item Observed stable refinement, though large iteration counts can inflate evaluations if prime indices spike too high.
    \item Reinforces the need for intelligent prime index rules.
\end{itemize}

\subsection{2D and 3D Benchmark Functions}

\subsubsection{Checkerboard-Floor (2D)}
\begin{itemize}
    \item \textbf{Discontinuous floor partitions.}
    \item \textbf{Result:} PAS effectively found the "floor boundary" optima, outperforming naive uniform grid.
\end{itemize}

\subsubsection{Stepped Hyperplane / Mixed 2D}
\begin{itemize}
    \item \textbf{Piecewise constant surfaces with abrupt transitions.}
    \item \textbf{Finding:} Hybrid PAS zoomed in on interesting boundaries quickly, skipping large homogeneous regions.
\end{itemize}

\subsection{Recursive Fractal Cliff Valley (3D)}

We introduced a highly fractal and oscillatory objective function designed to defeat simpler methods.
\begin{itemize}
    \item \textbf{Result:} PAS found deep minima more reliably than uniform searching.
    \item Large dimension + prime-based refinement can drive up evaluations but yield strong results.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig3_rfcv_visualization.png}
    \caption{Visualization of the Recursive Fractal Cliff Valley Function (RFCV), a challenging 3D benchmark designed to test optimization algorithms. The function features multiple local minima, cliffs, and valleys, making it particularly challenging for traditional optimization methods but well-suited for PAS.}
    \label{fig:rfcv_visualization}
\end{figure}

\subsection{Real-World Inspired Problems}

\subsubsection{Solar Farm Parameter Optimization}
\begin{itemize}
    \item \textbf{Objective:} Maximize energy production subject to discontinuous regulatory constraints.
    \item \textbf{Finding:} PAS overcame regulatory step changes and discovered near-optimal solutions with fewer evaluations than uniform scanning.
\end{itemize}

\subsubsection{Integer Parameter Discovery}
\begin{itemize}
    \item E.g., find integer $(a, b)$ for $a e^x - b x^2 = 18.2323423$.
    \item PAS quickly zeroed in on valid integer pairs, performing drastically better than brute force.
\end{itemize}

\section{Complexity Analysis}

\subsection{Prime Number Theorem Implications}

Given that the $n$-th prime is approximately $n \ln n$, if PAS increments the prime index linearly, the partition size grows sub-linearly in the domain size. For certain discrete integer searches, this can yield fewer evaluations than naive scanning.

\subsection{Iteration Count vs. Dimensional Blow-Up}

\begin{itemize}
    \item In $d$ dimensions, using prime $p$ per dimension means $p^d$ evaluations per iteration.
    \item For large $d$, this escalates quickly—exponential in naive form.
    \item \textbf{Mitigation:} Adaptive prime index capping, multi-level search, or partial dimension refinement can reduce overhead.
\end{itemize}

\subsection{Empirical Complexity}

From our experiments:
\begin{itemize}
    \item Near $\mathbf{O}(\log(N))$ iteration scaling in 1D tasks, if prime indices are well-managed.
    \item Performance in 2D/3D remains manageable with adaptive prime capping or dimension-specific prime updates.
\end{itemize}

\section{Related Work}

\subsection{Prime Number Applications in Optimization}

Several approaches have leveraged prime numbers in optimization and numerical methods:

\begin{itemize}
    \item \textbf{Kwak (1990)} used prime sample schemes in Galerkin approximations, finding improved stability compared to uniform grids for certain differential equation solvers \cite{kwak1990}.
    
    \item \textbf{Quasi-Monte Carlo} methods, including Sobol \cite{sobol1967} and Halton \cite{halton1960} sequences, use co-prime sampling but differ from PAS in their fixed sequence nature and lack of adaptive shrinking.
    
    \item \textbf{Direct Search Methods:} While Nelder-Mead and pattern search methods \cite{nocedal2006} are derivatives-free, they move through continuous space rather than using hierarchical prime-grid structures.
    
    \item \textbf{Grid Adaptivity:} Kelley's Implicit Filtering \cite{kelley1999} adjusts grid resolution but with uniform, not prime-driven, partitioning.
\end{itemize}

PAS's novelty lies in combining prime sampling with adaptive domain shrinking, creating a hierarchical search that excels in discontinuous spaces.

\section{Applications and Future Directions}

\subsection{Applications}

\begin{enumerate}
    \item \textbf{AI Hyperparameter Tuning}
    \begin{itemize}
        \item Non-smooth error surfaces in neural networks or ensemble methods.
        \item PAS can systematically test discrete + continuous parameter sets.
    \end{itemize}
    
    \item \textbf{Robotics and Control}
    \begin{itemize}
        \item Systems with contact modes or switching behavior that create discontinuities.
    \end{itemize}
    
    \item \textbf{Financial Modeling}
    \begin{itemize}
        \item Discrete transaction costs or tiered constraints—PAS handles discontinuous cost functions.
    \end{itemize}
    
    \item \textbf{Cryptographic Parameter Search}
    \begin{itemize}
        \item Searching large integer spaces for secure keys or prime-based cryptosystems—ironically fitting to use prime-based partitioning.
    \end{itemize}
    
    \item \textbf{Discontinuity Detection}
    \begin{itemize}
        \item By tracking large function jumps, PAS could localize discontinuities in black-box scenarios.
    \end{itemize}
\end{enumerate}

\subsection{Future Research}

\begin{itemize}
    \item \textbf{Convergence Theory}: Provide rigorous proofs of convergence rates in multi-dim.
    \item \textbf{Advanced Index Update Rules}: Explore feedback-based or error-proportional rules.
    \item \textbf{Hybrid Combinations}: Combine PAS with local search or Bayesian surrogate modeling.
    \item \textbf{High-Dimensional}: Investigate dimension reduction or partial prime adaptation for big-data scenarios.
    \item \textbf{Open-Source Ecosystem}: Expand existing PAS libraries, incorporate more test functions, and unify user community.
\end{itemize}

\section{Conclusion}

This paper introduced Prime-Adaptive Search (PAS), a novel optimization algorithm specifically designed for discontinuous and complex landscapes. Our analysis demonstrated PAS's robustness in situations where traditional gradient-based methods struggle. By combining prime-based grid sampling with adaptive domain refinement, PAS avoids the aliasing issues of uniform grids while efficiently concentrating computational effort in promising regions.

Through benchmark testing, we showed that PAS exhibits competitive performance on discontinuous functions, complex landscapes, and multi-modal problems. While gradient-based methods remain superior for smooth, well-behaved functions, PAS fills an important gap for non-smooth optimization problems encountered in various fields from engineering to machine learning.

Future work will explore higher-dimensional extensions, hybrid approaches combining PAS with local optimization, and specialized variants for constrained optimization problems. The DAPS implementation provided with this paper offers a practical tool for researchers and practitioners facing challenging optimization scenarios where traditional methods fall short.

\bibliographystyle{plain}
\begin{thebibliography}{9}
\bibitem{kwak1990} 
Kwak, J. (1990). 
\newblock Prime sample schemes in Galerkin approximations. 
\newblock {\em Journal of Approximation Theory}, XX, 123--145.

\bibitem{nocedal2006} 
Nocedal, J., \& Wright, S. (2006). 
\newblock {\em Numerical Optimization} (2nd ed.). 
\newblock Springer.

\bibitem{sobol1967} 
Sobol, I. (1967). 
\newblock On the distribution of points in a cube and the approximate evaluation of integrals. 
\newblock {\em USSR Computational Mathematics and Mathematical Physics}, 7, 86--112.

\bibitem{halton1960} 
Halton, J. H. (1960). 
\newblock On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals. 
\newblock {\em Numerische Mathematik}, 2, 84--90.

\bibitem{kelley1999} 
Kelley, C. T. (1999). 
\newblock {\em Iterative Methods for Optimization}. 
\newblock SIAM.
\end{thebibliography}

\end{document} 
